\documentclass[a4paper]{article}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{caption}
\titleclass{\subsubsubsection}{straight}[\subsection]

\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
\renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
\titlespacing*{\subsubsubsection}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
  {3.25ex \@plus1ex \@minus.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
  {3.25ex \@plus1ex \@minus .2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@paragraph{6}
\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
\def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
\def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
\makeatother

\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}

\usepackage{listings}             % Include the listings-package

\usepackage{xcolor}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ 
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Octave,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}


\usepackage[document]{ragged2e}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{graphicx}

\usepackage{lipsum}
\usepackage{setspace} % for \onehalfspacing and \singlespacing macros
\newcommand{\itab}[1]{\hspace{0em}\rlap{#1}}
\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
\onehalfspacing 
\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\singlespacing\small}

\usepackage[a4paper, total={7in, 8in}]{geometry}
%\geometry{legalpaper, margin=0.8in}
		
\date{\vspace{-5ex}}
\makeatletter
\def\@seccntformat#1{%
  \expandafter\ifx\csname c@#1\endcsname\c@section
  Section \thesection:
  \else
  \csname the#1\endcsname\quad
  \fi}
\makeatother
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,     
    urlcolor=cyan,
}
\renewcommand{\thesection}{\arabic{section}.}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\makeatletter
\def\@seccntformat#1{\@ifundefined{#1@cntformat}%
   {\csname the#1\endcsname\quad}%       default
   {\csname #1@cntformat\endcsname}}%    enable individual control

\makeatother




\begin{document}


\lstset{language=bash}          % Set your language (you can change the language for each code-block optionally)
\lstset{basicstyle = \ttfamily,columns=fullflexible}

\begin{figure}
\centering
{\includegraphics[scale=0.25]{logo.png}}
\end{figure}

\justify


\section{Quick Start}
\label{sec:quickstart}
 
\subsection{Administration QuickStart}
This document will cover installation and administration points of Singularity for multi-tenant HPC resources and will not cover usage of the command line tools, container usage, or example use cases.

\newpage


\tableofcontents
\newpage


\subsubsection{Installation}

There are two common ways to install Singularity, from source code and via binary packages. This document will explain the process of installation from source, and it will depend on your build host to have the appropriate development tools and packages installed. For Red Hat and derivatives, you should install the following \colorbox{lightgray}{yum} group to ensure you have an appropriately setup build server:
    
\begin{lstlisting}[frame=single]
$ sudo yum groupinstall "Development Tools"
\end{lstlisting}


\subsubsubsection{Downloading the Source}

You can download the source code either from the latest stable tarball release or via the GitHub master repository. Here is an example downloading and preparing the latest development code from GitHub:

\begin{lstlisting}[frame=single]
$ mkdir ~/git
$ cd ~/git
$ git clone https://github.com/singularityware/singularity.git
$ cd singularity
$ ./autogen.sh
\end{lstlisting}

Once you have downloaded the source, the following installation procedures will assume you are running from the root of the source directory.
\\[0.1in]

\subsubsubsection{Source Installation}

The following example demonstrates how to install Singularity into \colorbox{lightgray}{/usr/local}. You can install Singularity into any directory of your choosing, but you must ensure that the location you select supports programs running as \colorbox{lightgray}{SUID}. It is common for people to disable \colorbox{lightgray}{SUID} with the mount option \colorbox{lightgray}{nosuid} for various network mounted file systems. To ensure proper support, it is easiest to make sure you install Singularity to a local file system.\\[0.1in]

Assuming that \colorbox{lightgray}{/usr/local} is a local file system:


\textbf{NOTE: The \colorbox{lightgray}{make install} above must be run as root to have Singularity properly installed. Failure to install as root will cause Singularity to not function properly or have limited functionality when run by a non-root user.}\\[0.1in]

Also note that when you configure, \colorbox{lightgray}{squashfs-tools} is \textbf{not} required, however it is required for full functionality. You will see this message after the configuration:\\[0.1in]

\begin{lstlisting}[frame=single]
mksquashfs from squash-tools is required for full functionality
\end{lstlisting}

If you choose not to install \colorbox{lightgray}{squashfs-tools}, you will hit an error when your users try a pull from Docker Hub, for example.

\subsubsubsection{Prefix in special places (-localstatedir)}

As with most autotools-based build scripts, you are able to supply the \colorbox{lightgray}{-{}-prefix} argument to the configure script to change where Singularity will be installed. Care must be taken when this path is not a local filesystem or has atypical permissions. The local state directories used by Singularity at runtime will also be placed under the supplied \colorbox{lightgray}{-{}-prefix} and this will cause malfunction if the tree is read-only. You may also experience issues if this directory is shared between several hosts/nodes that might run Singularity simultaneously.\\[0.1in]

In such cases, you should specify the \colorbox{lightgray}{-{}-localstatedir} variable in addition to \colorbox{lightgray}{-{}-prefix}. This will override the prefix, instead placing the local state directories within the path explicitly provided. Ideally this should be within the local filesystem, specific to only a single host or node.\\[0.1in]

For example, the Makefile contains this variable by default:

\begin{lstlisting}[frame=single]
CONTAINER_OVERLAY = ${prefix}/var/singularity/mnt/overlay
\end{lstlisting}

By supplying the configure argument \colorbox{lightgray}{-{}-localstatedir=/some/other/place} Singularity will instead be built with the following. Note that \colorbox{lightgray}{\${prefix}/var} that has been replaced by the supplied value:

\begin{lstlisting}[frame=single]
CONTAINER_OVERLAY = /some/other/place/singularity/mnt/overlay
\end{lstlisting}

In the case of cluster nodes, you will need to ensure the following directories are created on all nodes, with \colorbox{lightgray}{root:root} ownership and \colorbox{lightgray}{0755} permissions:

\begin{lstlisting}[frame=single]
${localstatedir}/singularity/mnt
${localstatedir}/singularity/mnt/container
${localstatedir}/singularity/mnt/final
${localstatedir}/singularity/mnt/overlay
${localstatedir}/singularity/mnt/session
\end{lstlisting}

Singularity will fail to execute without these directories. They are normally created by the install make target; when using a local directory for  \colorbox{lightgray}{-{}-localstatedir} these will only be created on the node  \colorbox{lightgray}{make} is run on.



\subsubsubsection{Building an RPM directly from the source}

Singularity includes all of the necessary bits to properly create an RPM package directly from the source tree, and you can create an RPM by doing the following:


\begin{lstlisting}[frame=single]
$ ./configure
$ make dist
$ rpmbuild -ta singularity-*.tar.gz
\end{lstlisting}

Near the bottom of the build output you will see several lines like:


\begin{lstlisting}[frame=single]
...
Wrote: /home/gmk/rpmbuild/SRPMS/singularity-2.3.el7.centos.src.rpm
Wrote: /home/gmk/rpmbuild/RPMS/x86_64/singularity-2.3.el7.centos.x86_64.rpm
Wrote: /home/gmk/rpmbuild/RPMS/x86_64/singularity-devel-2.3.el7.centos.x86_64.rpm
Wrote: /home/gmk/rpmbuild/RPMS/x86_64/singularity-debuginfo-2.3.el7.centos.x86_64.rpm
...
\end{lstlisting}

You will want to identify the appropriate path to the binary RPM that you wish to install, in the above example the package we want to install is \colorbox{lightgray}{singularity-2.3.el7.centos.x86\_64.rpm}, and you should install it with the following command: 
\begin{lstlisting}[frame=single]
$ sudo yum install /home/gmk/rpmbuild/RPMS/x86_64/singularity-2.3.el7.centos.x86_64.rpm
\end{lstlisting}

Note: If you want to have the binary RPM install the files to an alternative location, you should define the environment variable ‘PREFIX’ (below) to suit your needs, and use the following command to build:

\begin{lstlisting}[frame=single]
$ PREFIX=/opt/singularity
$ rpmbuild -ta --define="_prefix $PREFIX" --define "_sysconfdir $PREFIX/etc" --define "_defaultdocdir $PREFIX/share" singularity-*.tar.gz
\end{lstlisting}

We recommend you look at our \hyperref[sec:security]{{\textcolor{cyan}{security admin guide}}} to get further information about container privileges and mounting.

\subsection{Security}
\label{sec:security}
\subsubsection{Container security paradigms}

First some background. Most container platforms operate on the premise, \textbf{trusted users running trusted containers}. This means that the primary UNIX account controlling the container platform is either “root” or user(s) that root has deputized (either via \colorbox{lightgray}{sudo} or given access to a control socket of a root owned daemon process).\\[0.1in]

Singularity on the other hand, operates on a different premise because it was developed for HPC type infrastructures where you have users, none of which are considered trusted. This means the paradigm is considerably different as we must support \textbf{untrusted users running untrusted containers}.

\subsubsection{Untrusted users running untrusted containers!}

This simple phrase describes the security perspective Singularity is designed with. And if you additionally consider the fact that running containers at all typically requires some level of privilege escalation, means that attention to security is of the utmost importance.\\[0.1in]

\subsubsubsection{Priviledge escalation is necessary for containerization!}

As mentioned, there are several containerization system calls and functions which are considered “privileged” in that they must be executed with a certain level of capability/privilege. To do this, all container systems must employ one of the following mechanisms:

\begin{enumerate}
\item \textbf{Limit usage to root:} Only allow the root user (or users granted \colorbox{lightgray}{sudo}) to run containers. This has the obvious limitation of not allowing arbitrary users the ability to run containers, nor does it allow users to run containers as themselves. Access to data, security data, and securing systems becomes difficult and perhaps impossible.
\item \textbf{Root owned daemon process:} Some container systems use a root owned daemon background process which manages the containers and spawns the jobs within the container. Implementations of this typically have an IPC control socket for communicating with this root owned daemon process and if you wish to allow trusted users to control the daemon, you must give them access to the control socket. This is the Docker model.
\item \textbf{SetUID:} Set UID is the “old school” UNIX method for running a particular program with escalated permission. While it is widely used due to it’s legacy and POSIX requirement, it lacks the ability to manage fine grained control of what a process can and can not do; a SetUID root program runs as root with all capabilities that comes with root. For this reason, SetUID programs are traditional targets for hackers.
\item \textbf{User Namespace:} The Linux kernel’s user namespace may allow a user to virtually become another user and run a limited set privileged system functions. Here the privilege escalation is managed via the Linux kernel which takes the onus off of the program. This is a new kernel feature and thus requires new kernels and not all distributions have equally adopted this technology.
\item \textbf{Capability Sets:} Linux handles permissions, access, and roles via capability sets. The root user has these capabilities automatically activated while non-privileged users typically do not have these capabilities enabled. You can enable and disable capabilities on a per process and per file basis (if allowed to do so).

\end{enumerate}


\subsubsubsection{How does Singularity do it?}

Singularity must allow users to run containers as themselves which rules out options 1 and 2 from the above list. Singularity supports the rest of the options to following degrees of functionally:

\begin{itemize}
\item \textbf{User Namespace:} Singularity supports the user namespace natively and can run completely unprivileged (“rootless”) since version 2.2 (October 2016) but features are severely limited. You will not be able to use container “images” and will be forced to only work with directory (sandbox) based containers. Additionally, as mentioned, the user namespace is not equally supported on all distribution kernels so don’t count on legacy system support and usability may vary.
\item \textbf{SetUID:} This is the default usage model for Singularity because it gives the most flexibility in terms of supported features and legacy compliance. It is also the most risky from a security perspective. For that reason, Singularity has been developed with transparency in mind. The code is written with attention to simplicity and readability and Singularity increases the effective permission set only when it is necessary, and drops it immediately (as can be seen with the --debug run flag). There have been several independent audits of the source code, and while they are not definitive, it is a good assurance.
\item \textbf{Capability Sets:} This is where Singularity is headed as an alternative to SetUID because it allows for much finer grained capability control and will support all of Singularity’s features. The downside is that it is not supported equally on shared file systems.

\end{itemize}

\subsubsection{Where are the Singularity priviledged components}

When you install Singularity as root, it will automatically setup the necessary files as SetUID (as of version 2.4, this is the default run mode). The location of these files is dependent on how Singularity was installed and the options passed to the \colorbox{lightgray}{configure} script. Assuming a default \colorbox{lightgray}{./configure} run which installs files into \colorbox{lightgray}{-{}-prefix} of \colorbox{lightgray}{/usr/local} you can find the SetUID programs as follows:

\begin{lstlisting}[frame=single]
$ find /usr/local/libexec/singularity/ -perm -4000
/usr/local/libexec/singularity/bin/start-suid
/usr/local/libexec/singularity/bin/action-suid
/usr/local/libexec/singularity/bin/mount-suid
\end{lstlisting}

Each of the binaries is named accordingly to the action that it is suited for, and generally, each handles the required privilege escalation necessary for Singularity to operate. What specifically requires escalated privileges?\\[0.1in]

\begin{enumerate}
\item Mounting (and looping) the Singularity container image
\item Creation of the necessary namespaces in the kernel
\item Binding host paths into the container
\end{enumerate}


Removing any of these SUID binaries or changing the permissions on them would cause Singularity to utilize the non-SUID workflows. Each file with \colorbox{lightgray}{*-suid} also has a non-suid equivalent:

\begin{lstlisting}[frame=single]
/usr/local/libexec/singularity/bin/start
/usr/local/libexec/singularity/bin/action
/usr/local/libexec/singularity/bin/mount
\end{lstlisting}

While most of these workflows will not properly function without the SUID components, we have provided these fall back executables for sites that wish to limit the SETUID capabilities to the bare essentials/minimum. To disable the SetUID portions of Singularity, you can either remove the above \colorbox{lightgray}{*-suid} files, or you can edit the setting for \colorbox{lightgray}{allow suid} at the top of the \colorbox{lightgray}{singularity.conf} file, which is typically located in \colorbox{lightgray}{\$PREFIX/etc/singularity/singularity.conf}.\\[0.1in]

\begin{lstlisting}[frame=single]
# ALLOW SETUID: [BOOL]
# DEFAULT: yes
# Should we allow users to utilize the setuid program flow within Singularity?
# note1: This is the default mode, and to utilize all features, this option
# will need to be enabled.
# note2: If this option is disabled, it will rely on the user namespace
# exclusively which has not been integrated equally between the different
# Linux distributions.
allow setuid = yes
\end{lstlisting}


You can also install Singularity as root without any of the SetUID components with the configure option \colorbox{lightgray}{-{}-disable-suid} as follows:

\begin{lstlisting}[frame=single]
$ ./configure --disable-suid --prefix=/usr/local
$ make
$ sudo make install
\end{lstlisting}

\subsubsection{Can I install Singularity as a user?}

Yes, but don’t expect all of the functions to work. If the SetUID components are not present, Singularity will attempt to use the “user namespace”. Even if the kernel you are using supports this namespace fully, you will still not be able to access all of the Singularity features.



\subsubsection{Container permissions and usage strategy}

As a system admin, you want to set up a configuration that is customized for your cluster or shared resource. In the following paragraphs, we will elaborate on this container permissions strategy, giving detail about which users are allowed to run containers, along with image curation and ownership.\\[0.1in]

These settings can all be found in the Singularity configuration file which is installed to \colorbox{lightgray}{\$PREFIX/etc/singularity/singularity.conf}. When running in a privileged mode, the configuration file \textbf{MUST} be owned by root and thus the system administrator always has the final control.



\subsubsubsection{controlling what kind of containers are allowed}

Singularity supports several different container formats:\\[0.1in]

\begin{itemize}
\item \textbf{squashfs:} Compressed immutable (read only) container images (default in version 2.4)
\item \textbf{extfs:} Raw file system writable container images
\item \textbf{dir:} Sandbox containers (chroot style directories)
\end{itemize}

Using the Singularity configuration file, you can control what types of containers Singularity will support:

\begin{lstlisting}[frame=single]
# ALLOW CONTAINER ${TYPE}: [BOOL]
# DEFAULT: yes
# This feature limits what kind of containers that Singularity will allow
# users to use (note this does not apply for root).
allow container squashfs = yes
allow container extfs = yes
allow container dir = yes

\end{lstlisting}

\subsubsubsection{limiting usage to specific container file owners}

One benefit of using container images is that they exist on the filesystem as any other file would. This means that POSIX permissions are mandatory. Here you can configure Singularity to only “trust” containers that are owned by a particular set of users.\\[0.1in]

\begin{lstlisting}[frame=single]
# LIMIT CONTAINER OWNERS: [STRING]
# DEFAULT: NULL
# Only allow containers to be used that are owned by a given user. If this
# configuration is undefined (commented or set to NULL), all containers are
# allowed to be used. This feature only applies when Singularity is running in
# SUID mode and the user is non-root.
#limit container owners = gmk, singularity, nobody

\end{lstlisting}

note: If you are in a high risk security environment, you may want to enable this feature. Trusting container images to users could allow a malicious user to modify an image either before or while being used and cause unexpected behavior from the kernel (e.g. a \href{https://en.wikipedia.org/wiki/Denial-of-service_attack}{DOS attack}). For more information, please see: \href{https://lwn.net/Articles/652468/}{https://lwn.net/Articles/652468/}

\subsubsubsection{limiting usage to specific paths}

The configuration file also gives you the ability to limit containers to specific paths. This is very useful to ensure that only trusted or blessed container’s are being used (it is also beneficial to ensure that containers are only being used on performant file systems).

\begin{lstlisting}[frame=single]
# LIMIT CONTAINER PATHS: [STRING]
# DEFAULT: NULL
# Only allow containers to be used that are located within an allowed path
# prefix. If this configuration is undefined (commented or set to NULL),
# containers will be allowed to run from anywhere on the file system. This
# feature only applies when Singularity is running in SUID mode and the user is
# non-root.
#limit container paths = /scratch, /tmp, /global
\end{lstlisting}


\subsubsection{Logging}
Singularity offers a very comprehensive auditing mechanism via the system log. For each command that is issued, it prints the UID, PID, and location of the command. For example, let’s see what happens if we shell into an image:

\begin{lstlisting}[frame=single]
$ singularity exec ubuntu true
$ singularity shell --home $HOME:/ ubuntu
Singularity: Invoking an interactive shell within container...

ERROR  : Failed to execv() /.singularity.d/actions/shell, continuing to /bin/sh: No such file or directory
ERROR  : What are you doing gmk, this is highly irregular!
ABORT  : Retval = 255
\end{lstlisting}

We can then peek into the system log to see what was recorded:

\begin{lstlisting}[frame=single]
Oct  5 08:51:12 localhost Singularity: action-suid (U=1000,P=32320)> USER=gmk, IMAGE='ubuntu', COMMAND='exec'
Oct  5 08:53:13 localhost Singularity: action-suid (U=1000,P=32311)> USER=gmk, IMAGE='ubuntu', COMMAND='shell'
Oct  5 08:53:13 localhost Singularity: action-suid (U=1000,P=32311)> Failed to execv() /.singularity.d/actions/shell, continuing to /bin/sh: No such file or directory
Oct  5 08:53:13 localhost Singularity: action-suid (U=1000,P=32311)> What are you doing gmk, this is highly irregular!
Oct  5 08:53:13 localhost Singularity: action-suid (U=1000,P=32311)> Retval = 255
\end{lstlisting}

\textbf{note: All errors are logged!}

\subsubsubsection{A peek into the SetUID program flow}

We can also add the \colorbox{lightgray}{-{}-debug} argument to any command itself at runtime to see everything that Singularity is doing. In this case we can run Singularity in debug mode and request use of the PID namespace so we can see what Singularity is doing there:
\begin{lstlisting}[frame=single]
$ singularity --debug shell --pid ubuntu
Enabling debugging
Ending argument loop
Singularity version: 2.3.9-development.gc35b753
Exec'ing: /usr/local/libexec/singularity/cli/shell.exec
Evaluating args: '--pid ubuntu'
\end{lstlisting}

(snipped to PID namespace implementation)

\begin{lstlisting}[frame=single]
DEBUG   [U=1000,P=30961]   singularity_runtime_ns_pid()              Using PID namespace: CLONE_NEWPID
DEBUG   [U=1000,P=30961]   singularity_runtime_ns_pid()              Virtualizing PID namespace
DEBUG   [U=1000,P=30961]   singularity_registry_get()                Returning NULL on 'DAEMON_START'
DEBUG   [U=1000,P=30961]   prepare_fork()                            Creating parent/child coordination pipes.
VERBOSE [U=1000,P=30961]   singularity_fork()                        Forking child process
DEBUG   [U=1000,P=30961]   singularity_priv_escalate()               Temporarily escalating privileges (U=1000)
DEBUG   [U=0,P=30961]      singularity_priv_escalate()               Clearing supplementary GIDs.
DEBUG   [U=0,P=30961]      singularity_priv_drop()                   Dropping privileges to UID=1000, GID=1000 (8 supplementary GIDs)
DEBUG   [U=0,P=30961]      singularity_priv_drop()                   Restoring supplementary groups
DEBUG   [U=1000,P=30961]   singularity_priv_drop()                   Confirming we have correct UID/GID
VERBOSE [U=1000,P=30961]   singularity_fork()                        Hello from parent process
DEBUG   [U=1000,P=30961]   install_generic_signal_handle()           Assigning generic sigaction()s
DEBUG   [U=1000,P=30961]   install_generic_signal_handle()           Creating generic signal pipes
DEBUG   [U=1000,P=30961]   install_sigchld_signal_handle()           Assigning SIGCHLD sigaction()
DEBUG   [U=1000,P=30961]   install_sigchld_signal_handle()           Creating sigchld signal pipes
DEBUG   [U=1000,P=30961]   singularity_fork()                        Dropping permissions
DEBUG   [U=0,P=30961]      singularity_priv_drop()                   Dropping privileges to UID=1000, GID=1000 (8 supplementary GIDs)
DEBUG   [U=0,P=30961]      singularity_priv_drop()                   Restoring supplementary groups
DEBUG   [U=1000,P=30961]   singularity_priv_drop()                   Confirming we have correct UID/GID
DEBUG   [U=1000,P=30961]   singularity_signal_go_ahead()             Sending go-ahead signal: 0
DEBUG   [U=1000,P=30961]   wait_child()                              Parent process is waiting on child process
DEBUG   [U=0,P=1]          singularity_priv_drop()                   Dropping privileges to UID=1000, GID=1000 (8 supplementary GIDs)
DEBUG   [U=0,P=1]          singularity_priv_drop()                   Restoring supplementary groups
DEBUG   [U=1000,P=1]       singularity_priv_drop()                   Confirming we have correct UID/GID
VERBOSE [U=1000,P=1]       singularity_fork()                        Hello from child process
DEBUG   [U=1000,P=1]       singularity_wait_for_go_ahead()           Waiting for go-ahead signal
DEBUG   [U=1000,P=1]       singularity_wait_for_go_ahead()           Received go-ahead signal: 0
VERBOSE [U=1000,P=1]       singularity_registry_set()                Adding value to registry: 'PIDNS_ENABLED' = '1'
\end{lstlisting}

(snipped to end)

\begin{lstlisting}[frame=single]
DEBUG   [U=1000,P=1]       envar_set()                               Unsetting environment variable: SINGULARITY_APPNAME
DEBUG   [U=1000,P=1]       singularity_registry_get()                Returning value from registry: 'COMMAND' = 'shell'
LOG     [U=1000,P=1]       main()                                    USER=gmk, IMAGE='ubuntu', COMMAND='shell'
INFO    [U=1000,P=1]       action_shell()                            Singularity: Invoking an interactive shell within container...

DEBUG   [U=1000,P=1]       action_shell()                            Exec'ing /.singularity.d/actions/shell
Singularity ubuntu:~> 

\end{lstlisting}


Not only do I see all of the configuration options that I (probably forgot about) previously set, I can trace the entire flow of Singularity from the first execution of an action (shell) to the final shell into the container. Each line also describes what is the effective UID running the command, what is the PID, and what is the function emitting the debug message.

\subsubsubsection{A peek into the "rootless" program flow}

The above snippet was using the default SetUID program flow with a container image file named “ubuntu”. For comparison, if we also use the \colorbox{lightgray}{-{}-userns} flag, and snip in the same places, you can see how the effective UID is never escalated, but we have the same outcome using a sandbox directory (chroot) style container.

\begin{lstlisting}[frame=single]
$ singularity -d shell --pid --userns ubuntu.dir/
Enabling debugging
Ending argument loop
Singularity version: 2.3.9-development.gc35b753
Exec'ing: /usr/local/libexec/singularity/cli/shell.exec
Evaluating args: '--pid --userns ubuntu.dir/'
\end{lstlisting}


(snipped to PID namespace implementation, same place as above)\\[0.1in]
\begin{lstlisting}[frame=single]
DEBUG   [U=1000,P=32081]   singularity_runtime_ns_pid()              Using PID namespace: CLONE_NEWPID
DEBUG   [U=1000,P=32081]   singularity_runtime_ns_pid()              Virtualizing PID namespace
DEBUG   [U=1000,P=32081]   singularity_registry_get()                Returning NULL on 'DAEMON_START'
DEBUG   [U=1000,P=32081]   prepare_fork()                            Creating parent/child coordination pipes.
VERBOSE [U=1000,P=32081]   singularity_fork()                        Forking child process
DEBUG   [U=1000,P=32081]   singularity_priv_escalate()               Not escalating privileges, user namespace enabled
DEBUG   [U=1000,P=32081]   singularity_priv_drop()                   Not dropping privileges, user namespace enabled
VERBOSE [U=1000,P=32081]   singularity_fork()                        Hello from parent process
DEBUG   [U=1000,P=32081]   install_generic_signal_handle()           Assigning generic sigaction()s
DEBUG   [U=1000,P=32081]   install_generic_signal_handle()           Creating generic signal pipes
DEBUG   [U=1000,P=32081]   install_sigchld_signal_handle()           Assigning SIGCHLD sigaction()
DEBUG   [U=1000,P=32081]   install_sigchld_signal_handle()           Creating sigchld signal pipes
DEBUG   [U=1000,P=32081]   singularity_signal_go_ahead()             Sending go-ahead signal: 0
DEBUG   [U=1000,P=32081]   wait_child()                              Parent process is waiting on child process
DEBUG   [U=1000,P=1]       singularity_priv_drop()                   Not dropping privileges, user namespace enabled
VERBOSE [U=1000,P=1]       singularity_fork()                        Hello from child process
DEBUG   [U=1000,P=1]       singularity_wait_for_go_ahead()           Waiting for go-ahead signal
DEBUG   [U=1000,P=1]       singularity_wait_for_go_ahead()           Received go-ahead signal: 0
VERBOSE [U=1000,P=1]       singularity_registry_set()                Adding value to registry: 'PIDNS_ENABLED' = '1'

\end{lstlisting}

(snipped to end)

\begin{lstlisting}[frame=single]
DEBUG   [U=1000,P=1]       envar_set()                               Unsetting environment variable: SINGULARITY_APPNAME
DEBUG   [U=1000,P=1]       singularity_registry_get()                Returning value from registry: 'COMMAND' = 'shell'
LOG     [U=1000,P=1]       main()                                    USER=gmk, IMAGE='ubuntu.dir', COMMAND='shell'
INFO    [U=1000,P=1]       action_shell()                            Singularity: Invoking an interactive shell within container...

DEBUG   [U=1000,P=1]       action_shell()                            Exec'ing /.singularity.d/actions/shell
Singularity ubuntu.dir:~> whoami
gmk
Singularity ubuntu.dir:~> 
\end{lstlisting}

Here you can see that the output and functionality is very similar, but we never increased any privilege and none of the \colorbox{lightgray}{*-suid} program flow was utilized. We had to use a chroot style directory container (as images are not supported with the user namespace, but you can clearly see that the effective UID never had to change to run this container.\\[0.1in]

note: Singularity can natively create and manage chroot style containers just like images! The above image was created using the command: \colorbox{lightgray}{singularity build ubuntu.dir docker://ubuntu:latest}

\subsubsection{Summary}
Singularity supports multiple modes of operation to meet your security needs. For most HPC centers, and general usage scenarios, the default run mode is most effective and featurefull. For the security critical implementations, the user namespace workflow maybe a better option. It becomes a balance security and functionality (the most secure systems do nothing).

\subsection{The Singularity Config File}

When Singularity is running via the SUID pathway, the configuration \textbf{must} be owned by the root user otherwise Singularity will error out. This ensures that the system administrators have direct say as to what functions the users can utilize when running as root. If Singularity is installed as a non-root user, the SUID components are not installed, and the configuration file can be owned by the user (but again, this will limit functionality).\\[0.1in]

The Configuration file can be found at \colorbox{lightgray}{\$SYSCONFDIR/singularity/singularity.conf}. The template in the repository is located at \colorbox{lightgray}{etc/singularity.conf}. It is generally self documenting but there are several things to pay special attention to:

\subsubsection{Parameters}

\subsubsubsection{ALLOW SETUID (boolean, default='yes')}

This parameter toggles the global ability to execute the SETUID (SUID) portion of the code if it exists. As mentioned earlier, if the SUID features are disabled, various Singularity features will not function (e.g. mounting of the Singularity image file format).\\[0.1in]

You can however disable SUID support \textbf{iff} (if and only if) you do not need to use the default Singularity image file format and if your kernel supports user namespaces and you choose to use user namespaces.\\[0.1in]

note: as of the time of this writing, the user namespace is rather buggy\\[0.1in]

\subsubsubsection{ALLOW PID NS (boolean, default=’yes’)}

While the PID namespace is a neat feature, it does not have much practical usage in an HPC context so it is recommended to disable this if you are running on an HPC system where a resource manager is involved as it has been known to cause confusion on some kernels with enforcement of user limits.\\[0.1in]

Even if the PID namespace is enabled by the system administrator here, it is not implemented by default when running containers. The user will have to specify they wish to implement un-sharing of the PID namespace as it must fork a child process.

\subsubsubsection{ENABLE OVERLAY (boolean, default=’no’)}

The overlay file system creates a writable substrate to create bind points if necessary. This feature is very useful when implementing bind points within containers where the bind point may not already exist so it helps with portability of containers. Enabling this option has been known to cause some kernels to panic as this feature maybe present within a kernel, but has not proved to be stable as of the time of this writing (e.g. the Red Hat 7.2 kernel).

\subsubsubsection{CONFIG PASSWD,GROUP,RESOLV\_CONF (boolean, default=’yes’)}
All of these options essentially do the same thing for different files within the container. This feature updates the described file (\colorbox{lightgray}{/etc/passwd}, \colorbox{lightgray}{/etc/group}, and \colorbox{lightgray}{/etc/resolv.conf} respectively) to be updated dynamically as the container is executed. It uses binds and modifies temporary files such that the original files are not manipulated.


\subsubsubsection{MOUNT PROC,SYS,DEV,HOME,TMP (boolean, default=’yes’)}

These configuration options control the mounting of these file systems within the container and of course can be overridden by the system administrator (e.g. the system admin decides not to include the /dev tree inside the container). In most useful cases, these are all best to leave enabled.

\subsubsubsection{MOUNT HOSTFS (boolean, default=’no’)}
This feature will parse the host’s mounted file systems and attempt to replicate all mount points within the container. This maybe a desirable feature for the lazy, but it is generally better to statically define what bind points you wish to encapsulate within the container by hand (using the below “bind path” feature).


\subsubsubsection{BIND PATH (string)}

With this configuration directive, you can specify any number of bind points that you want to extend from the host system into the container. Bind points on the host file system must be either real files or directories (no special files supported at this time). If the overlayFS is not supported on your host, or if \colorbox{lightgray}{enable overlay = no} in this configuration file, a bind point must exist for the file or directory within the container. \\[0.1in]

The syntax for this consists of a bind path source and an optional bind path destination separated by a colon. If no bind path destination is specified, the bind path source is used also as the destination.



\subsubsubsection{USER BIND CONTROL (boolean, default=’yes’)}

In addition to the system bind points as specified within this configuration file, you may also allow users to define their own bind points inside the container. This feature is used via multiple command line arguments (e.g.  \colorbox{lightgray}{-{}-bind},  \colorbox{lightgray}{-{}-scratch}, and  \colorbox{lightgray}{-{}-home}) so disabling user bind control will also disable those command line options.\\[0.1in]

Singularity will automatically disable this feature if the host does not support the prctl option  \colorbox{lightgray}{PR\_SET\_NO\_NEW\_PRIVS}. In addition,  \colorbox{lightgray}{enable overlay} must be set to  \colorbox{lightgray}{yes} and the host system must support overlayFS (generally kernel versions 3.18 and later) for users to bind host directories to bind points that do not already exist in the container.



\subsubsubsection{AUTOFS BUG PATH (string)}

With some versions of autofs, Singularity will fail to run with a “Too many levels of symbolic links” error. This error happens by way of a user requested bind (done with -B/–bind) or one specified via the configuration file. To handle this, you will want to specify those paths using this directive. For example:\\[0.1in]
\begin{lstlisting}[frame=single]
autofs bug path = /share/PI
\end{lstlisting}


\subsubsection{Logging}

In order to facilitate monitoring and auditing, Singularity will syslog() every action and error that takes place to the \colorbox{lightgray}{LOCAL0} syslog facility. You can define what to do with those logs in your syslog configuration.



\subsubsection{Loop Devices}

Singularity images have  \colorbox{lightgray}{ext3} file systems embedded within them, and thus to mount them, we need to convert the raw file system image (with variable offset) to a block device. To do this, Singularity utilizes the  \colorbox{lightgray}{/dev/loop*} block devices on the host system and manages the devices programmatically within Singularity itself. Singularity also uses the  \colorbox{lightgray}{LO\_FLAGS\_AUTOCLEAR} loop device  \colorbox{lightgray}{ioctl()} flag which tells the kernel to automatically free the loop device when there are no more open file descriptors to the device itself.\\[0.1in]

Earlier versions of Singularity managed the loop devices via a background watchdog process, but since version 2.2 we leverage the  \colorbox{lightgray}{LO\_FLAGS\_AUTOCLEAR} functionality and we forego the watchdog process. Unfortunately, this means that some older Linux distributions are no longer supported (e.g. RHEL <= 5).\\[0.1in]

Given that loop devices are consumable (there are a limited number of them on a system), Singularity attempts to be smart in how loop devices are allocated. For example, if a given user executes a specific container it will bind that image to the next available loop device automatically. If that same user executes another command on the same container, it will use the loop device that has already been allocated instead of binding to another loop device. Most Linux distributions only support 8 loop devices by default, so if you find that you have a lot of different users running Singularity containers, you may need to increase the number of loop devices that your system supports by doing the following:\\[0.1in]

Edit or create the file  \colorbox{lightgray}{/etc/modprobe.d/loop.conf} and add the following line:

\begin{lstlisting}[frame=single]
options loop max_loop=128
\end{lstlisting}

After making this change, you should be able to reboot your system or unload/reload the loop device as root using the following commands:

\begin{lstlisting}[frame=single]
# modprobe -r loop
# modprobe loop
\end{lstlisting}

\subsection{Container Checks}

New to Singularity 2.4 is the ability to, on demand, run container “checks,” which can be anything from a filter for sensitive information, to an analysis of content on the filesystem. Checks are installed with Singularity, managed by the administrator, and \href{http://singularity.lbl.gov/docs-user-checks}{available to the user}.

\subsubsection{What is a check?}
Broadly, a check is a script that is run over a mounted filesystem, primary with the purpose of checking for some security issue. This process is tightly controlled, meaning that the script names in the \href{https://github.com/singularityware/singularity/tree/development/libexec/helpers/checks}{checks} folder are hard coded into the script \href{https://github.com/singularityware/singularity/blob/development/libexec/helpers/check.sh}{check.sh}. The flow of checks is the following:
\\[0.1in]

\begin{itemize}
\item the user calls \colorbox{lightgray}{singularity check container.img} to invoke \href{https://github.com/singularityware/singularity/blob/development/libexec/cli/check.exec}{check.exec}
\item specification of  \colorbox{lightgray}{-{}-low} (3),  \colorbox{lightgray}{-{}-med} (2), or  \colorbox{lightgray}{-{}-high} (1) sets the level to perform. The level is a filter, meaning that a level of 3 will include 3,2,1, and a level of 1 (high) will only call checks of high priority.
\item specification of  \colorbox{lightgray}{-t/--tag} will allow the user (or execution script) to specify a kind of check. This is primarily to allow for extending the checks to do other types of things. For example, for this initial batch, these are all considered  \colorbox{lightgray}{default} checks. The \href{https://github.com/singularityware/singularity/blob/development/libexec/cli/check.help}{check.help} displays examples of how the user specifies a tag:
\end{itemize}

\begin{lstlisting}[frame=single]
    # Perform all default checks, these are the same
    $ singularity check ubuntu.img
    $ singularity check --tag default ubuntu.img

    # Perform checks with tag "clean"
    $ singularity check --tag clean ubuntu.img

\end{lstlisting}


\subsubsubsection{Adding a Check}

A check should be a bash (or other) script that will perform some action. The following is required:\\[0.1in]

\textbf{Relative to SINGULARITY\_ROOTFS} The script must perform check actions relative to \colorbox{lightgray}{\$SINGULARITY\_ROOTFS}. For example, in python you might change directory to this location:
\begin{lstlisting}[frame=single]
import os
base = os.environ["SINGULARITY_ROOTFS"]
os.chdir(base)
\end{lstlisting}


or do the same in bash:

\begin{lstlisting}[frame=single]
cd $SINGULARITY_ROOTFS
ls $SINGULARITY_ROOTFS/var
\end{lstlisting}

Since we are doing a mount, all checks must be static relative to this base, otherwise you are likely checking the host system.\\[0.1in]

\textbf{Verbose} The script should indicate any warning/message to the user if the check is found to have failed. If pass, the check’s name and status will be printed, with any relevant information. For more thorough checking, you might want to give more verbose output.\\[0.1in]

\textbf{Return Code} The script return code of “success” is defined in \href{http://singularity.lbl.gov/check.sh}{check.sh}, and other return codes are considered not success. When a non success return code is found, the rest of the checks continue running, and no action is taken. We might want to give some admin an ability to specify a check, a level, and prevent continuation of the build/bootstrap given a fail.\\[0.1in]

\textbf{Check.sh} The script level, path, and tags should be added to \href{http://singularity.lbl.gov/check.sh}{check.sh} in the following format:

\begin{lstlisting}[frame=single]
##################################################################################
# CHECK SCRIPTS
##################################################################################

#        [SUCCESS] [LEVEL]  [SCRIPT]                                                                         [TAGS]
execute_check    0    HIGH  "bash $SINGULARITY_libexecdir/singularity/helpers/checks/1-hello-world.sh"       security
execute_check    0     LOW  "python $SINGULARITY_libexecdir/singularity/helpers/checks/2-cache-content.py"   clean
execute_check    0    HIGH  "python $SINGULARITY_libexecdir/singularity/helpers/checks/3-cve.py"             security

\end{lstlisting}

The function \colorbox{lightgray}{execute\_check} will compare the level (\colorbox{lightgray}{[LEVEL]}) with the user specified (or default) \colorbox{lightgray}{SINGULARITY\_CHECKLEVEL} and execute the check only given it is under the specified threshold, and (not yet implemented) has the relevant tag. The success code is also set here with \colorbox{lightgray}{[SUCCESS]}. Currently, we aren’t doing anything with \colorbox{lightgray}{[TAGS]} and thus perform all checks.

\subsubsection{How to tell users?}

If you add a custom check that you want for your users to use, you should tell them about it. Better yet, \href{https://github.com/singularityware/singularity/issues}{tell us}  about it so it can be integrated into the Singularity software for others to use.

\subsection{Troubleshooting}

This section will help you debug (from the system administrator’s perspective) Singularity.

\subsubsection{Not installed correctly, or installed to a non-compatible location}

Singularity must be installed by root into a location that allows for SUID programs to be executed (as described above in the installation section of this manual). If you fail to do that, you may have user’s reporting one of the following error conditions:\\[0.1in]

\begin{lstlisting}[frame=single]
ERROR  : Singularity must be executed in privileged mode to use images
ABORT  : Retval = 255
\end{lstlisting}
\begin{lstlisting}[frame=single]
ERROR  : User namespace not supported, and program not running privileged.
ABORT  : Retval = 255
\end{lstlisting}
\begin{lstlisting}[frame=single]
ABORT  : This program must be SUID root
ABORT  : Retval = 255
\end{lstlisting}

If one of these errors is reported, it is best to check the installation of Singularity and ensure that it was properly installed by the root user onto a local file system.

\section{Installation Environments}
\subsection{Singularity on HPC}

One of the architecturally defined features in Singularity is that it can execute containers like they are native programs or scripts on a host computer. As a result, integration with schedulers is simple and runs exactly as you would expect. All standard input, output, error, pipes, IPC, and other communication pathways that locally running programs employ are synchronized with the applications running locally within the container.\\[0.1in]

Additionally, because Singularity is not emulating a full hardware level virtualization paradigm, there is no need to separate out any sandboxed networks or file systems because there is no concept of user-escalation within a container. Users can run Singularity containers just as they run any other program on the HPC resource.\\[0.1in]

\subsubsection{Workflows}

We are in the process of developing Singularity Hub, which will allow for generation of workflows using Singularity containers in an online interface, and easy deployment on standard research clusters (e.g., SLURM, SGE). Currently, the Singularity core software is installed on the following research clusters, meaning you can run Singularity containers as part of your jobs:

\begin{itemize}
\item The \href{http://sherlock.stanford.edu/}{Sherlock cluster} at \href{https://srcc.stanford.edu/}{Stanford University}
\item \href{https://www.xsede.org/news/-/news/item/7624}{SDSC Comet and Gordon} (XSEDE)
\item \href{http://docs.massive.org.au/index.html}{MASSIVE M1 M2 and M3} (Monash University and Australian National Merit Allocation Scheme)

\end{itemize}

\subsubsubsection{Integration with MPI}
Another result of the Singularity architecture is the ability to properly integrate with the Message Passing Interface (MPI). Work has already been done for out of the box compatibility with Open MPI (both in Open MPI v2.1.x as well as part of Singularity). The Open MPI/Singularity workflow works as follows:

\begin{enumerate}
\item mpirun is called by the resource manager or the user directly from a shell
\item Open MPI then calls the process management daemon (ORTED)
\item The ORTED process launches the Singularity container requested by the mpirun command
\item Singularity builds the container and namespace environment
\item Singularity then launches the MPI application within the container
\item The MPI application launches and loads the Open MPI libraries
\item The Open MPI libraries connect back to the ORTED process via the Process Management Interface (PMI)
\item At this point the processes within the container run as they would normally directly on the host.
\end{enumerate}

This entire process happens behind the scenes, and from the user’s perspective running via MPI is as simple as just calling mpirun on the host as they would normally.\\[0.1in]
Below are example snippets of building and installing OpenMPI into a container and then running an example MPI program through Singularity.


\paragraph{Tutorials}

\begin{itemize}
\item \href{http://singularity.lbl.gov/tutorial-gpu-drivers-open-mpi-mtls}{Using Host libraries: GPU drivers and OpenMPI BTLs
}
\end{itemize}

\paragraph{MPI Development Example}

\textbf{What are supported Open MPI Version(s)?} To achieve proper container’ized Open MPI support, you should use Open MPI version 2.1. There are however three caveats:

\begin{enumerate}
\item Open MPI 1.10.x may work but we expect you will need exactly matching version of PMI and Open MPI on both host and container (the 2.1 series should relax this requirement)
\item Open MPI 2.1.0 has a bug affecting compilation of libraries for some interfaces (particularly Mellanox interfaces using libmxm are known to fail). If your in this situation you should use the master branch of Open MPI rather than the release.
\item Using Open MPI 2.1 does not magically allow your container to connect to networking fabric libraries in the host. If your cluster has, for example, an infiniband network you still need to install OFED libraries into the container. Alternatively you could bind mount both Open MPI and networking libraries into the container, but this could run afoul of glib compatibility issues (its generally OK if the container glibc is more recent than the host, but not the other way around)

\end{enumerate}

\paragraph{Code Example using Open MPI 2.1.0 Stable}\mbox{}\\
\begin{lstlisting}[frame=single]
$ # Include the appropriate development tools into the container (notice we are calling
$ # singularity as root and the container is writable)
$ sudo singularity exec -w /tmp/Centos-7.img yum groupinstall "Development Tools"
$
$ # Obtain the development version of Open MPI
$ wget https://www.open-mpi.org/software/ompi/v2.1/downloads/openmpi-2.1.0.tar.bz2
$ tar jtf openmpi-2.1.0.tar.bz2
$ cd openmpi-2.1.0
$
$ singularity exec /tmp/Centos-7.img ./configure --prefix=/usr/local
$ singularity exec /tmp/Centos-7.img make
$
$ # Install OpenMPI into the container (notice now running as root and container is writable)
$ sudo singularity exec -w -B /home /tmp/Centos-7.img make install
$
$ # Build the OpenMPI ring example and place the binary in this directory
$ singularity exec /tmp/Centos-7.img mpicc examples/ring_c.c -o ring
$
$ # Install the MPI binary into the container at /usr/bin/ring
$ sudo singularity copy /tmp/Centos-7.img ./ring /usr/bin/
$
$ # Run the MPI program within the container by calling the MPIRUN on the host
$ mpirun -np 20 singularity exec /tmp/Centos-7.img /usr/bin/ring

\end{lstlisting}

\paragraph{Code Example using Open MPI git master}

The previous example (using the Open MPI 2.1.0 stable release) should work fine on most hardware but if you have an issue, try running the example below (using the Open MPI Master branch):

\begin{lstlisting}[frame=single]
$ # Include the appropriate development tools into the container (notice we are calling
$ # singularity as root and the container is writable)
$ sudo singularity exec -w /tmp/Centos-7.img yum groupinstall "Development Tools"
$
$ # Clone the OpenMPI GitHub master branch in current directory (on host)
$ git clone https://github.com/open-mpi/ompi.git
$ cd ompi
$
$ # Build OpenMPI in the working directory, using the tool chain within the container
$ singularity exec /tmp/Centos-7.img ./autogen.pl
$ singularity exec /tmp/Centos-7.img ./configure --prefix=/usr/local
$ singularity exec /tmp/Centos-7.img make
$
$ # Install OpenMPI into the container (notice now running as root and container is writable)
$ sudo singularity exec -w -B /home /tmp/Centos-7.img make install
$
$ # Build the OpenMPI ring example and place the binary in this directory
$ singularity exec /tmp/Centos-7.img mpicc examples/ring_c.c -o ring
$
$ # Install the MPI binary into the container at /usr/bin/ring
$ sudo singularity copy /tmp/Centos-7.img ./ring /usr/bin/
$
$ # Run the MPI program within the container by calling the MPIRUN on the host
$ mpirun -np 20 singularity exec /tmp/Centos-7.img /usr/bin/ring


Process 0 sending 10 to 1, tag 201 (20 processes in ring)
Process 0 sent to 1
Process 0 decremented value: 9
Process 0 decremented value: 8
Process 0 decremented value: 7
Process 0 decremented value: 6
Process 0 decremented value: 5
Process 0 decremented value: 4
Process 0 decremented value: 3
Process 0 decremented value: 2
Process 0 decremented value: 1
Process 0 decremented value: 0
Process 0 exiting
Process 1 exiting
Process 2 exiting
Process 3 exiting
Process 4 exiting
Process 5 exiting
Process 6 exiting
Process 7 exiting
Process 8 exiting
Process 9 exiting
Process 10 exiting
Process 11 exiting
Process 12 exiting
Process 13 exiting
Process 14 exiting
Process 15 exiting
Process 16 exiting
Process 17 exiting
Process 18 exiting
Process 19 exiting

\end{lstlisting}

\subsection{Image Environment}
\subsubsection{Directory access}

By default Singularity tries to create a seamless user experience between the host and the container. To do this, Singularity makes various locations accessible within the container automatically. For example, the user’s home directory is always bound into the container as is /tmp and /var/tmp. Additionally your current working directory (cwd/pwd) is also bound into the container iff it is not an operating system directory or already accessible via another mount. For almost all cases, this will work flawlessly as follows:

\begin{lstlisting}[frame=single]
$ pwd
/home/gmk/demo
$ singularity shell container.img 
Singularity/container.img> pwd
/home/gmk/demo
Singularity/container.img> ls -l debian.def 
-rw-rw-r--. 1 gmk gmk 125 May 28 10:35 debian.def
Singularity/container.img> exit
$ 
\end{lstlisting}

For directory binds to function properly, there must be an existing target endpoint within the container (just like a mount point). This means that if your home directory exists in a non-standard base directory like “/foobar/username” then the base directory “/foobar” must already exist within the container.\\[0.1in]

Singularity will not create these base directories! You must enter the container with the \colorbox{lightgray}{-{}-writable} option being set, and create the directory manually.



\subsubsubsection{Current Working Directory}

Singularity will try to replicate your current working directory within the container. Sometimes this is straight forward and possible, other times it is not (e.g. if the base dir of your current working directory does not exist). In that case, Singularity will retain the file descriptor to your current directory and change you back to it. If you do a ‘pwd’ within the container, you may see some weird things. For example:

\begin{lstlisting}[frame=single]
$ pwd
/foobar
$ ls -l
total 0
-rw-r--r--. 1 root root 0 Jun  1 11:32 mooooo
$ singularity shell ~/demo/container.img 
WARNING: CWD bind directory not present: /foobar
Singularity/container.img> pwd
(unreachable)/foobar
Singularity/container.img> ls -l
total 0
-rw-r--r--. 1 root root 0 Jun  1 18:32 mooooo
Singularity/container.img> exit
$ 

\end{lstlisting}

But notice how even though the directory location is not resolvable, the directory contents are available.



\subsubsection{Standard IO and pipes}

Singularity automatically sends and receives all standard IO from the host to the applications within the container to facilitate expected behavior from the interaction between the host and the container. For example:
\begin{lstlisting}[frame=single]
$ cat debian.def | singularity exec container.img grep 'MirrorURL'
MirrorURL "http://ftp.us.debian.org/debian/"
$ 
Making changes to the container (writable)
By default, containers are accessed as read only. This is both to enable parallel container execution (e.g. MPI). To enter a container using exec, run, or shell you must pass the --writable flag in order to open the image as read/writable.

\end{lstlisting}

\subsubsection{Containing the container}

By providing the argument \colorbox{lightgray}{-{}-contain} to \colorbox{lightgray}{exec}, \colorbox{lightgray}{run} or \colorbox{lightgray}{shell} you will find that shared directories are no longer shared. For example, the user’s home directory is writable, but it is non-persistent between non-overlapping runs.

\subsection{License}
\begin{lstlisting}[frame=single]
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:
 
(1) Redistributions of source code must retain the above copyright notice,
this list of conditions and the following disclaimer.
 
(2) Redistributions in binary form must reproduce the above copyright notice,
this list of conditions and the following disclaimer in the documentation
and/or other materials provided with the distribution.
 
(3) Neither the name of the University of California, Lawrence Berkeley
National Laboratory, U.S. Dept. of Energy nor the names of its contributors
may be used to endorse or promote products derived from this software without
specific prior written permission.
 
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

You are under no obligation whatsoever to provide any bug fixes, patches, or
upgrades to the features, functionality or performance of the source code
("Enhancements") to anyone; however, if you choose to make your Enhancements
available either publicly, or directly to Lawrence Berkeley National
Laboratory, without imposing a separate written license agreement for such
Enhancements, then you hereby grant the following license: a  non-exclusive,
royalty-free perpetual license to install, use, modify, prepare derivative
works, incorporate into other computer software, distribute, and sublicense
such enhancements or derivative works thereof, in binary and source code form.

If you have questions about your rights to use or distribute this software,
please contact Berkeley Lab's Innovation & Partnerships Office at
IPO@lbl.gov.

NOTICE.  This Software was developed under funding from the U.S. Department of
Energy and the U.S. Government consequently retains certain rights. As such,
the U.S. Government has been granted for itself and others acting on its
behalf a paid-up, nonexclusive, irrevocable, worldwide license in the Software
to reproduce, distribute copies to the public, prepare derivative works, and
perform publicly and display publicly, and to permit other to do so. 
\end{lstlisting}

\subsubsection{In layman terms...}

In addition to the (already widely used and very free open source) standard BSD 3 clause license, there is also wording specific to contributors which ensures that we have permission to release, distribute and include a particular contribution, enhancement, or fix as part of Singularity proper. For example any contributions submitted will have the standard BSD 3 clause terms (unless specifically and otherwise stated) and that the contribution is comprised of original new code that the contributor has authority to contribute.


\end{document}
